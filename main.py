import os
import json
import subprocess
import argparse
from pathlib import Path
from typing import List, Tuple, Dict
from paddleocr import PaddleOCR
import cv2
import shutil
from tqdm import tqdm
import numpy as np
from difflib import SequenceMatcher

try:
    from opencc import OpenCC
    HAS_OPENCC = True
except ImportError:
    HAS_OPENCC = False


def get_groups_mean(arr: list, tolerance: float = 20) -> float:
    """
    è®¡ç®—åˆ†ç»„åçš„å¹³å‡å€¼ã€‚
    å¯¹ç»™å®šæ•°ç»„è¿›è¡Œåˆ†ç»„ï¼Œæ¯ç»„å†…çš„å…ƒç´ ä¸ç»„å†…æœ€å°å…ƒç´ çš„å·®å€¼ä¸å¤§äºtoleranceã€‚
    ç„¶åè®¡ç®—æœ€å¤§ç»„çš„å¹³å‡å€¼ä½œä¸ºç»“æœã€‚

    Args:
        arr: è¾“å…¥çš„æ•°å­—åˆ—è¡¨
        tolerance: åˆ†ç»„çš„å·®å€¼å®¹å¿åº¦ï¼Œé»˜è®¤ä¸º20

    Returns:
        æœ€å¤§ç»„çš„å¹³å‡å€¼
    """
    if not arr:
        return 0

    arr_sorted = sorted(arr)
    groups = []
    current_group = [arr_sorted[0]]

    for i in range(1, len(arr_sorted)):
        if abs(arr_sorted[i] - current_group[0]) <= tolerance:
            current_group.append(arr_sorted[i])
        else:
            groups.append(current_group)
            current_group = [arr_sorted[i]]

    groups.append(current_group)
    max_group = max(groups, key=len)
    return np.mean(max_group)


def text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦

    Args:
        text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
        text2: ç¬¬äºŒä¸ªæ–‡æœ¬

    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0.0 åˆ° 1.0)
    """
    if not text1 or not text2:
        return 0.0

    # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦
    return SequenceMatcher(None, text1, text2).ratio()


class VideoSubtitleExtractor:
    """è§†é¢‘å­—å¹•æå–å™¨"""

    def __init__(self, output_dir: str = "output", extract_fps: int = 30,
                 subtitle_region_bottom: float = 0.1, subtitle_region_top: float = 0.45,
                 use_gpu: bool = True, start_time: float = 0, duration: float = None):
        """
        åˆå§‹åŒ–å­—å¹•æå–å™¨

                Args:
                    output_dir: è¾“å‡ºç›®å½•
                    extract_fps: æå–å¸§ç‡ï¼ˆæ¯ç§’æå–å¤šå°‘å¸§ï¼‰ï¼Œé»˜è®¤30
                    subtitle_region_bottom: å­—å¹•åŒºåŸŸåº•éƒ¨ä½ç½®ï¼ˆè·ç¦»åº•éƒ¨çš„ç™¾åˆ†æ¯”ï¼‰ï¼Œé»˜è®¤0.1ï¼ˆ10%ï¼‰
                    subtitle_region_top: å­—å¹•åŒºåŸŸé¡¶éƒ¨ä½ç½®ï¼ˆè·ç¦»åº•éƒ¨çš„ç™¾åˆ†æ¯”ï¼‰ï¼Œé»˜è®¤0.45ï¼ˆ45%ï¼‰
                    use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿï¼Œé»˜è®¤True
                    start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0ï¼ˆä»å¤´å¼€å§‹ï¼‰
                    duration: å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤Noneï¼ˆå¤„ç†åˆ°è§†é¢‘ç»“æŸï¼‰
                """
        self.output_dir = Path(output_dir)
        self.frames_dir = self.output_dir / "frames"
        self.extract_fps = extract_fps
        self.subtitle_region_bottom = subtitle_region_bottom
        self.subtitle_region_top = subtitle_region_top
        self.use_gpu = use_gpu
        self.start_time = start_time
        self.duration = duration

        # åˆå§‹åŒ–ç¹ä½“è½¬ç®€ä½“è½¬æ¢å™¨
        if HAS_OPENCC:
            self.cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“
            print("å·²å¯ç”¨ç¹ä½“è½¬ç®€ä½“åŠŸèƒ½")
        else:
            self.cc = None
            print("æœªå®‰è£…openccï¼Œå°†è·³è¿‡ç¹ä½“è½¬ç®€ä½“è½¬æ¢")

        # æ£€æµ‹GPUå¯ç”¨æ€§å¹¶è®¾ç½®è®¾å¤‡
        if use_gpu:
            gpu_available = self._check_gpu_availability()
            if not gpu_available:
                print("è­¦å‘Š: æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                self.use_gpu = False
                device = 'cpu'
            else:
                print(f"ä½¿ç”¨GPUåŠ é€Ÿè¿›è¡ŒOCRè¯†åˆ«")
                device = 'gpu:0'  # ä½¿ç”¨ç¬¬0å—GPU
        else:
            print("ä½¿ç”¨CPUæ¨¡å¼è¿›è¡ŒOCRè¯†åˆ«")
            device = 'cpu'

        # ä½¿ç”¨ PP-OCRv5 æ¨¡å‹ï¼Œä¼˜åŒ–å‚æ•°ä»¥æé«˜è¯†åˆ«æ•ˆæœ
        self.ocr = PaddleOCR(
            use_textline_orientation=True,  # æ–°ç‰ˆæœ¬æ¨èå‚æ•°ï¼ˆåŸuse_angle_clsï¼‰
            lang='ch',
            text_rec_score_thresh=0.7,      # è¯†åˆ«é˜ˆå€¼ï¼Œå¹³è¡¡æ•æ„Ÿåº¦å’Œå™ªå£°è¿‡æ»¤
            text_det_box_thresh=0.5,        # æ£€æµ‹é˜ˆå€¼ï¼Œé€‚ä¸­è®¾ç½®
            text_det_thresh=0.01,            # åƒç´ é˜ˆå€¼ï¼Œé€‚ä¸­æ•æ„Ÿåº¦
            text_det_unclip_ratio=2.5,      # æ‰©å¼ ç³»æ•°ï¼Œæ‰©å¤§æ–‡æœ¬æ£€æµ‹åŒºåŸŸ
            text_detection_model_name='PP-OCRv5_server_det',
            text_recognition_model_name='PP-OCRv5_server_rec',
            ocr_version='PP-OCRv5',
            device=device  # PaddleOCR 3.2.0+ ä½¿ç”¨deviceå‚æ•°æŒ‡å®šè®¡ç®—è®¾å¤‡
        )

    def _check_gpu_availability(self) -> bool:
        """
        æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨

        Returns:
            GPUæ˜¯å¦å¯ç”¨
        """
        try:
            import paddle
            return paddle.device.is_compiled_with_cuda() and paddle.device.cuda.device_count() > 0
        except Exception as e:
            print(f"GPUæ£€æµ‹å¤±è´¥: {e}")
            return False

    def get_video_info(self, video_path: str) -> Dict:
        """
        ä½¿ç”¨ffprobeè·å–è§†é¢‘ä¿¡æ¯

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸
        """
        print(f"æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯: {video_path}")

        cmd = [
            'ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_streams',
            '-show_format',
            video_path
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            video_info = json.loads(result.stdout)

            # æŸ¥æ‰¾è§†é¢‘æµ
            video_stream = None
            for stream in video_info.get('streams', []):
                if stream.get('codec_type') == 'video':
                    video_stream = stream
                    break

            if not video_stream:
                raise ValueError("æœªæ‰¾åˆ°è§†é¢‘æµ")

            # è§£æå¸§ç‡
            fps_str = video_stream.get('r_frame_rate', '25/1')
            fps_parts = fps_str.split('/')
            fps = float(fps_parts[0]) / float(fps_parts[1])

            # è·å–æ€»æ—¶é•¿
            duration = float(video_info.get('format', {}).get('duration', 0))

            info = {
                'fps': fps,
                'duration': duration,
                'width': video_stream.get('width'),
                'height': video_stream.get('height')
            }

            print(f"è§†é¢‘ä¿¡æ¯: FPS={info['fps']:.2f}, æ—¶é•¿={info['duration']:.2f}ç§’, "
                  f"åˆ†è¾¨ç‡={info['width']}x{info['height']}")

            return info

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"ffprobeæ‰§è¡Œå¤±è´¥: {e.stderr}")
        except json.JSONDecodeError as e:
            raise RuntimeError(f"è§£æffprobeè¾“å‡ºå¤±è´¥: {e}")

    def extract_frames(self, video_path: str, fps: float) -> int:
        """
        ä½¿ç”¨ffmpegæå–è§†é¢‘å¸§ï¼Œå¹¶è£å‰ªåˆ°å­—å¹•åŒºåŸŸ

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            fps: è§†é¢‘å¸§ç‡

        Returns:
            æå–çš„å¸§æ•°
        """
        # æ„å»ºå¤„ç†æ—¶é—´ä¿¡æ¯
        time_info = []
        if self.start_time > 0:
            time_info.append(f"ä»{self.start_time:.1f}ç§’å¼€å§‹")
        if self.duration:
            time_info.append(f"å¤„ç†{self.duration:.1f}ç§’")
        else:
            time_info.append("å¤„ç†åˆ°è§†é¢‘ç»“æŸ")

        time_desc = "ï¼Œ".join(time_info) if time_info else "å¤„ç†æ•´ä¸ªè§†é¢‘"
        print(f"æ­£åœ¨æå–è§†é¢‘å¸§ï¼ˆæ¯ç§’{self.extract_fps}å¸§ï¼‰ï¼Œå¹¶è£å‰ªåˆ°å­—å¹•åŒºåŸŸ...")
        print(f"å¤„ç†èŒƒå›´: {time_desc}")

        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = self.get_video_info(video_path)
        width = video_info['width']
        height = video_info['height']
        total_duration = video_info['duration']

        # éªŒè¯æ—¶é—´å‚æ•°
        if self.start_time >= total_duration:
            raise ValueError(f"å¼€å§‹æ—¶é—´({self.start_time}s)è¶…è¿‡è§†é¢‘æ€»æ—¶é•¿({total_duration:.1f}s)")

        # è®¡ç®—å®é™…å¤„ç†æ—¶é•¿
        actual_duration = self.duration
        if actual_duration:
            if self.start_time + actual_duration > total_duration:
                actual_duration = total_duration - self.start_time
                print(f"æ³¨æ„: å¤„ç†æ—¶é•¿å·²è°ƒæ•´ä¸º{actual_duration:.1f}ç§’ï¼ˆåˆ°è§†é¢‘ç»“æŸï¼‰")

        # è®¡ç®—è£å‰ªåŒºåŸŸï¼ˆä¸ocr_framesä¸­çš„è®¡ç®—ä¿æŒä¸€è‡´ï¼‰
        bottom_y = int(height * (1 - self.subtitle_region_top))  # é¡¶éƒ¨è¾¹ç•Œ
        top_y = int(height * (1 - self.subtitle_region_bottom))  # åº•éƒ¨è¾¹ç•Œ
        crop_height = top_y - bottom_y

        print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, æ€»æ—¶é•¿={total_duration:.1f}ç§’")
        print(f"å­—å¹•åŒºåŸŸ: y={bottom_y} åˆ° y={top_y} (é«˜åº¦={crop_height}px, å æ¯”{self.subtitle_region_bottom*100:.0f}%-{self.subtitle_region_top*100:.0f}%)")

        # åˆ›å»ºå¸§è¾“å‡ºç›®å½•
        if self.frames_dir.exists():
            shutil.rmtree(self.frames_dir)
        self.frames_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ffmpegæå–å¸§ï¼Œå¹¶ä½¿ç”¨crop filterè£å‰ªåˆ°å­—å¹•åŒºåŸŸ
        # cropè¯­æ³•: crop=width:height:x:y
        output_pattern = str(self.frames_dir / "frame_%06d.jpg")

        cmd = [
            'ffmpeg',
        ]

        # æ·»åŠ å¼€å§‹æ—¶é—´å‚æ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if self.start_time > 0:
            cmd.extend(['-ss', str(self.start_time)])

        cmd.extend(['-i', video_path])

        # æ·»åŠ æ—¶é•¿å‚æ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if actual_duration:
            cmd.extend(['-t', str(actual_duration)])

        cmd.extend([
            '-vf', f'fps={self.extract_fps},crop={width}:{crop_height}:0:{bottom_y}',  # å…ˆè®¾ç½®fpså†è£å‰ª
            '-q:v', '2',  # å›¾ç‰‡è´¨é‡
            output_pattern,
            '-y'  # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
        ])

        try:
            subprocess.run(cmd, capture_output=True, text=True, check=True)

            # ç»Ÿè®¡æå–çš„å¸§æ•°
            frame_count = len(list(self.frames_dir.glob("frame_*.jpg")))
            print(f"æˆåŠŸæå–å¹¶è£å‰ª {frame_count} å¸§")

            return frame_count

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"ffmpegæ‰§è¡Œå¤±è´¥: {e.stderr}")

    def detect_subtitle_region(self, frame_path: str) -> Tuple[int, int, int, int]:
        """
        æ£€æµ‹å­—å¹•åŒºåŸŸï¼ˆé€šå¸¸åœ¨è§†é¢‘åº•éƒ¨ï¼‰

        Args:
            frame_path: å¸§å›¾ç‰‡è·¯å¾„

        Returns:
            å­—å¹•åŒºåŸŸåæ ‡ (x, y, width, height)
        """
        img = cv2.imread(frame_path)
        height, width = img.shape[:2]

        # å‡è®¾å­—å¹•åœ¨åº•éƒ¨20%çš„åŒºåŸŸ
        subtitle_height = int(height * 0.2)
        subtitle_y = height - subtitle_height

        return (0, subtitle_y, width, subtitle_height)

    def preview_subtitle_region(self, video_path: str, frame_times: list = None):
        """
        é¢„è§ˆå­—å¹•åŒºåŸŸï¼Œç”Ÿæˆå¤šä¸ªæ—¶é—´ç‚¹çš„æ ‡æ³¨å›¾ç‰‡

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            frame_times: è¦é¢„è§ˆçš„æ—¶é—´ç‚¹åˆ—è¡¨ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©å¤šä¸ªæ—¶é—´ç‚¹
        """
        print(f"ç”Ÿæˆå­—å¹•åŒºåŸŸé¢„è§ˆ...")

        # è·å–è§†é¢‘æ—¶é•¿
        try:
            duration_cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
                          '-of', 'default=noprint_wrappers=1:nokey=1', video_path]
            result = subprocess.run(duration_cmd, capture_output=True, text=True, check=True)
            duration = float(result.stdout.strip())
            print(f"è§†é¢‘æ—¶é•¿: {duration:.1f}ç§’")
        except:
            print("è­¦å‘Š: æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´ç‚¹")
            duration = 60

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´ç‚¹ï¼Œè‡ªåŠ¨é€‰æ‹©å¤šä¸ªæ—¶é—´ç‚¹
        if frame_times is None:
            # é€‰æ‹©å¼€å¤´ã€1/4ã€1/2ã€3/4ã€ç»“å°¾å‰çš„æ—¶é—´ç‚¹
            frame_times = [
                10,  # å¼€å¤´10ç§’
                duration * 0.25,  # 1/4å¤„
                duration * 0.5,   # ä¸­é—´
                duration * 0.75,  # 3/4å¤„
                max(10, duration - 10)  # ç»“å°¾å‰10ç§’
            ]
            # å»é‡å¹¶æ’åº
            frame_times = sorted(list(set([t for t in frame_times if 0 < t < duration])))

        print(f"å°†é¢„è§ˆ {len(frame_times)} ä¸ªæ—¶é—´ç‚¹:")
        for i, t in enumerate(frame_times, 1):
            print(f"  {i}. {t:.1f}ç§’")
        print()

        preview_files = []
        crop_files = []

        for idx, time_sec in enumerate(frame_times):
            # æå–æŒ‡å®šæ—¶é—´ç‚¹çš„å¸§
            temp_frame = self.frames_dir / f"preview_frame_{idx}.jpg"
            cmd = [
                'ffmpeg',
                '-ss', str(time_sec),
                '-i', video_path,
                '-frames:v', '1',
                str(temp_frame),
                '-y'
            ]

            try:
                subprocess.run(cmd, capture_output=True, text=True, check=True)

                # è¯»å–å›¾ç‰‡
                img = cv2.imread(str(temp_frame))
                if img is None:
                    print(f"âš  æ— æ³•è¯»å–ç¬¬{idx+1}ä¸ªé¢„è§ˆå¸§ (æ—¶é—´: {time_sec:.1f}s)")
                    continue

                height, width = img.shape[:2]

                # è®¡ç®—å­—å¹•åŒºåŸŸ
                bottom_y = int(height * (1 - self.subtitle_region_top))
                top_y = int(height * (1 - self.subtitle_region_bottom))

                # åˆ›å»ºæ ‡æ³¨å›¾ç‰‡
                preview_img = img.copy()

                # ç»˜åˆ¶å­—å¹•åŒºåŸŸçŸ©å½¢ï¼ˆç»¿è‰²ï¼‰
                cv2.rectangle(preview_img, (0, bottom_y), (width, top_y), (0, 255, 0), 3)

                # æ·»åŠ æ–‡å­—æ ‡æ³¨
                font = cv2.FONT_HERSHEY_SIMPLEX
                cv2.putText(preview_img, f'Subtitle Region (Time: {time_sec:.1f}s)',
                           (10, bottom_y - 10), font, 1, (0, 255, 0), 2)
                cv2.putText(preview_img, f'Bottom: {self.subtitle_region_bottom*100:.0f}%',
                           (10, top_y + 30), font, 0.8, (0, 255, 0), 2)
                cv2.putText(preview_img, f'Top: {self.subtitle_region_top*100:.0f}%',
                           (10, bottom_y - 40), font, 0.8, (0, 255, 0), 2)

                # è£å‰ªå­—å¹•åŒºåŸŸ
                subtitle_crop = img[bottom_y:top_y, :]

                # ä¿å­˜å®Œæ•´é¢„è§ˆå›¾
                preview_path = self.output_dir / f"preview_region_{idx+1}_{int(time_sec)}s.jpg"
                cv2.imwrite(str(preview_path), preview_img)
                preview_files.append(preview_path)

                # ä¿å­˜è£å‰ªåçš„å­—å¹•åŒºåŸŸ
                crop_path = self.output_dir / f"preview_crop_{idx+1}_{int(time_sec)}s.jpg"
                cv2.imwrite(str(crop_path), subtitle_crop)
                crop_files.append(crop_path)

                print(f"âœ“ ç¬¬{idx+1}ä¸ªé¢„è§ˆ (æ—¶é—´: {time_sec:.1f}s) å·²ç”Ÿæˆ")

            except subprocess.CalledProcessError as e:
                print(f"âœ— æå–ç¬¬{idx+1}ä¸ªé¢„è§ˆå¸§å¤±è´¥ (æ—¶é—´: {time_sec:.1f}s): {e.stderr}")

        # æ‰“å°æ€»ç»“
        print(f"\n{'='*60}")
        print(f"é¢„è§ˆç”Ÿæˆå®Œæˆï¼å…± {len(preview_files)} ä¸ªæ—¶é—´ç‚¹")
        print(f"{'='*60}")
        print(f"\nå®Œæ•´æ ‡æ³¨å›¾ç‰‡ ({len(preview_files)}å¼ ):")
        for f in preview_files:
            print(f"  - {f}")

        print(f"\nå­—å¹•è£å‰ªå›¾ç‰‡ ({len(crop_files)}å¼ ):")
        for f in crop_files:
            print(f"  - {f}")

        if preview_files:
            # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾è·å–å°ºå¯¸ä¿¡æ¯
            img = cv2.imread(str(preview_files[0]))
            height, width = img.shape[:2]
            bottom_y = int(height * (1 - self.subtitle_region_top))
            top_y = int(height * (1 - self.subtitle_region_bottom))

            print(f"\nå­—å¹•åŒºåŸŸä¿¡æ¯ï¼š")
            print(f"  è§†é¢‘å°ºå¯¸: {width}x{height}")
            print(f"  å­—å¹•åŒºåŸŸ: y={bottom_y} åˆ° y={top_y} (é«˜åº¦={top_y-bottom_y}px)")
            print(f"  å æ¯”: åº•éƒ¨{self.subtitle_region_bottom*100:.0f}% åˆ° {self.subtitle_region_top*100:.0f}%")

    def ocr_frames(self) -> Dict[str, Dict]:
        """
        å¯¹æ‰€æœ‰å¸§è¿›è¡ŒOCRè¯†åˆ«ï¼ˆå¸§å·²ç»æ˜¯è£å‰ªåçš„å­—å¹•åŒºåŸŸï¼‰

        Returns:
            è¯†åˆ«ç»“æœå­—å…¸ï¼Œkeyä¸ºå¸§è·¯å¾„ï¼ŒvalueåŒ…å«æ–‡æœ¬ã€è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯
        """
        frame_files = sorted(self.frames_dir.glob("frame_*.jpg"))
        results = {}

        # ä½¿ç”¨è¿›åº¦æ¡
        for idx, frame_path in enumerate(tqdm(frame_files, desc="OCR Processing", unit="FPS")):
            # è¯»å–å›¾ç‰‡ï¼ˆå·²ç»æ˜¯è£å‰ªåçš„å­—å¹•åŒºåŸŸï¼‰
            img = cv2.imread(str(frame_path))
            if img is None:
                tqdm.write(f"è­¦å‘Š: æ— æ³•è¯»å–å¸§ {frame_path}")
                continue

            # ä½¿ç”¨æŠ½è±¡çš„æ ¸å¿ƒOCRè¯†åˆ«æ–¹æ³•
            debug_print = (idx == 0) or (frame_path.name == "frame_000708.jpg")  # ç¬¬ä¸€å¸§æˆ–ç‰¹å®šå¸§æ‰“å°è°ƒè¯•ä¿¡æ¯
            if debug_print:
                tqdm.write(f"æ‰¹é‡è¯†åˆ«è°ƒè¯•: å¤„ç†å¸§ {frame_path.name}")

            try:
                # ç‰¹æ®Šè°ƒè¯•ï¼šä¸ºframe_000708ä¿å­˜è°ƒè¯•å›¾ç‰‡
                if frame_path.name == "frame_000708.jpg":
                    debug_img_path = f"debug_{frame_path.name}"
                    cv2.imwrite(debug_img_path, img)
                    tqdm.write(f"ğŸ” ä¿å­˜è°ƒè¯•å›¾ç‰‡: {debug_img_path}, å°ºå¯¸: {img.shape}")

                ocr_result = self._ocr_image(img, debug_print=debug_print)

                # ä»æ–‡ä»¶åæå–çœŸå®çš„å¸§ç´¢å¼•
                frame_name = frame_path.stem  # frame_000708
                real_frame_index = int(frame_name.split('_')[1])  # 708

                # ç‰¹æ®Šè°ƒè¯•ï¼šè¯¦ç»†è¾“å‡ºframe_000708çš„OCRç»“æœ
                if frame_path.name == "frame_000708.jpg":
                    tqdm.write(f"ğŸ” frame_000708 OCRåŸå§‹ç»“æœ:")
                    tqdm.write(f"  - è¯†åˆ«åˆ°çš„æ–‡æœ¬æ•°é‡: {len(ocr_result['texts'])}")
                    for i, text_info in enumerate(ocr_result['texts']):
                        tqdm.write(f"  - æ–‡æœ¬{i+1}: \"{text_info['text']}\" -> \"{text_info['simplified_text']}\" (ç½®ä¿¡åº¦: {text_info['score']:.3f})")
                    tqdm.write(f"  - åˆå¹¶åæ–‡æœ¬: \"{ocr_result['combined_text']}\"")

                # å¦‚æœæœ‰è¯†åˆ«ç»“æœï¼Œä¿å­˜åˆ°å­—å…¸ä¸­
                if ocr_result['texts']:
                    # æŒ‰ x åæ ‡æ’åºï¼ˆä»å·¦åˆ°å³ï¼‰
                    text_items = sorted(ocr_result['texts'], key=lambda x: x['box'][0])

                    # ä½¿ç”¨å·²ç»å¤„ç†å¥½çš„åˆå¹¶æ–‡æœ¬
                    combined_text = ocr_result['combined_text']

                    if debug_print:
                        tqdm.write(f"æ‰¹é‡è¯†åˆ«è°ƒè¯•: æœ€ç»ˆæ–‡æœ¬ '{combined_text}'")

                    # è®¡ç®—æ•´ä½“è¾¹ç•Œæ¡†
                    if text_items:
                        all_boxes = [item['box'] for item in text_items]
                        xmin = min(box[0] for box in all_boxes)
                        ymin = min(box[1] for box in all_boxes)
                        xmax = max(box[2] for box in all_boxes)
                        ymax = max(box[3] for box in all_boxes)

                        results[str(frame_path)] = {
                            'text': combined_text,
                            'box': [xmin, ymin, xmax, ymax],
                            'frame_index': real_frame_index,  # ä½¿ç”¨çœŸå®çš„å¸§ç´¢å¼•
                            'items': text_items  # ä¿ç•™åŸå§‹æ–‡æœ¬é¡¹
                        }

            except Exception as e:
                if debug_print:
                    tqdm.write(f"æ‰¹é‡è¯†åˆ«è°ƒè¯•: OCRå¤„ç†å¤±è´¥ {e}")
                continue

        return results

    def check_ocr_result(self, ocr_result: Dict[str, Dict], video_info: Dict) -> Dict[str, Dict]:
        """
        æ ¡éªŒå¹¶æ•´åˆOCRè¯†åˆ«ç»“æœ

        å‚è€ƒ: https://github.com/chenwr727/SubErase-Translate-Embed

        ä¸»è¦åŠŸèƒ½ï¼š
        1. ç»Ÿè®¡å­—å¹•çš„ä¸­å¿ƒä½ç½®å’Œé«˜åº¦
        2. è¿‡æ»¤æ‰ä¸åœ¨å­—å¹•åŒºåŸŸçš„æ–‡æœ¬
        3. åˆå¹¶åŒä¸€å¸§å†…ç›¸é‚»çš„æ–‡æœ¬
        4. å¡«å……è¿ç»­å¸§ä¹‹é—´çš„ç©ºç™½

        Args:
            ocr_result: OCRè¯†åˆ«ç»“æœå­—å…¸
            video_info: è§†é¢‘ä¿¡æ¯ï¼ˆåŒ…å«åˆ†è¾¨ç‡ï¼‰

        Returns:
            æ ¡éªŒå’Œæ•´åˆåçš„OCRç»“æœ
        """
        if not ocr_result:
            return {}

        # è·å–å›¾åƒå°ºå¯¸ï¼ˆä»è£å‰ªåçš„å¸§ï¼‰
        first_frame = next(iter(ocr_result.keys()))
        img = cv2.imread(first_frame)
        if img is None:
            return ocr_result

        img_height, img_width = img.shape[:2]

        # é…ç½®å‚æ•° - å¤§å¹…æ”¾å®½å®¹å¿åº¦ä»¥å‡å°‘è¯¯è¿‡æ»¤
        width_delta = img_width * 0.5   # æ°´å¹³ä½ç½®å®¹å¿åº¦ï¼ˆ50%ï¼ŒåŸ30%ï¼‰
        height_delta = img_height * 0.4  # å‚ç›´ä½ç½®å®¹å¿åº¦ï¼ˆ40%ï¼ŒåŸ10%ï¼‰
        groups_tolerance = img_height * 0.5  # åˆ†ç»„å®¹å¿åº¦ï¼ˆ50%ï¼ŒåŸ5%ï¼‰

        x_center_frame = img_width / 2

        # ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡å­—å¹•çš„ä¸­å¿ƒä½ç½®å’Œé«˜åº¦
        center_list = []
        word_height_list = []

        for frame_path, value in tqdm(ocr_result.items(), desc="ç»Ÿè®¡å­—å¹•ä½ç½®", unit="å¸§"):
            xmin, ymin, xmax, ymax = value['box']
            x_center = (xmin + xmax) / 2
            y_center = (ymin + ymax) / 2

            # åªç»Ÿè®¡é è¿‘æ°´å¹³ä¸­å¿ƒçš„æ–‡æœ¬
            if x_center - width_delta < x_center_frame < x_center + width_delta:
                center_list.append(y_center)
                word_height_list.append(ymax - ymin)

        if not center_list:
            return ocr_result

        # ä½¿ç”¨åˆ†ç»„ç»Ÿè®¡æ‰¾åˆ°æœ€å¸¸è§çš„å­—å¹•ä½ç½®å’Œé«˜åº¦
        center = get_groups_mean(center_list, groups_tolerance)
        word_height = get_groups_mean(word_height_list, groups_tolerance)

        print(f"  æ£€æµ‹åˆ°å­—å¹•ä¸­å¿ƒä½ç½®: y={center:.0f}px (å®¹å¿Â±{height_delta:.0f}px)")
        print(f"  æ£€æµ‹åˆ°å­—å¹•å¹³å‡é«˜åº¦: {word_height:.0f}px")

        # ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤å¹¶åˆå¹¶åŒä¸€å¸§å†…çš„æ–‡æœ¬
        filtered_result = {}

        for frame_path, value in tqdm(ocr_result.items(), desc="è¿‡æ»¤OCRç»“æœ", unit="å¸§"):
            xmin, ymin, xmax, ymax = value['box']
            y_center = (ymin + ymax) / 2
            x_center = (xmin + xmax) / 2
            text_height = ymax - ymin

            # æ£€æŸ¥æ˜¯å¦åœ¨å­—å¹•åŒºåŸŸå†…
            if (center - height_delta < y_center < center + height_delta and
                word_height - groups_tolerance <= text_height <= word_height + groups_tolerance):

                # æ£€æŸ¥å¤šä¸ªæ–‡æœ¬é¡¹ï¼Œçœ‹æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆå¹¶
                if 'items' in value and len(value['items']) > 1:
                    # åˆå¹¶ç›¸é‚»çš„æ–‡æœ¬é¡¹
                    merged_text = value['text']
                    merged_box = value['box']
                else:
                    merged_text = value['text']
                    merged_box = value['box']

                filtered_result[frame_path] = {
                    'text': merged_text,
                    'box': merged_box,
                    'frame_index': value['frame_index']
                }

        # ç¬¬ä¸‰æ­¥ï¼šå¡«å……è¿ç»­å¸§ä¹‹é—´çš„ç©ºç™½
        if not filtered_result:
            return {}

        # æŒ‰å¸§ç´¢å¼•æ’åº
        sorted_frames = sorted(filtered_result.items(), key=lambda x: x[1]['frame_index'])

        final_result = {}
        min_duration_frames = int(self.extract_fps * 0.3)  # æœ€å°æŒç»­æ—¶é—´0.3ç§’

        for i in range(len(sorted_frames)):
            frame_path, value = sorted_frames[i]
            frame_idx = value['frame_index']
            text = value['text']

            final_result[frame_path] = value

            # å¦‚æœä¸æ˜¯æœ€åä¸€å¸§ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¡«å……
            if i < len(sorted_frames) - 1:
                next_frame_path, next_value = sorted_frames[i + 1]
                next_frame_idx = next_value['frame_index']
                next_text = next_value['text']

                # å¦‚æœæ–‡æœ¬ç›¸åŒä¸”å¸§é—´éš”ä¸å¤§ï¼Œå¡«å……ä¸­é—´çš„å¸§
                if text == next_text and (next_frame_idx - frame_idx) <= min_duration_frames:
                    # å¡«å……ä¸­é—´çš„å¸§
                    for fill_idx in range(frame_idx + 1, next_frame_idx):
                        fill_frame_name = f"frame_{fill_idx:06d}.jpg"
                        fill_frame_path = str(self.frames_dir / fill_frame_name)

                        if os.path.exists(fill_frame_path):
                            final_result[fill_frame_path] = {
                                'text': text,
                                'box': value['box'],
                                'frame_index': fill_idx
                            }

        print(f"  è¿‡æ»¤å‰: {len(ocr_result)} å¸§ï¼Œè¿‡æ»¤å: {len(filtered_result)} å¸§ï¼Œå¡«å……å: {len(final_result)} å¸§")

        return final_result

    def merge_subtitle_segments(self, ocr_results: Dict[str, Dict], similarity_threshold: float = 0.8, max_gap_seconds: float = 0.2) -> List[Dict]:
        """
        åˆå¹¶è¿ç»­ç›¸åŒæˆ–ç›¸ä¼¼çš„å­—å¹•æ®µ

        æ”¹è¿›çš„åˆå¹¶ç®—æ³•ï¼Œè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
        1. ç©ºæ–‡æœ¬å¼ºåˆ¶åˆ†å‰²é—®é¢˜ - ä½¿ç”¨æ—¶é—´é—´éš”åˆ¤æ–­
        2. ç›¸ä¼¼åº¦é˜ˆå€¼ä¼˜åŒ– - é™ä½åˆ°0.75ä»¥å¤„ç†æ›´å¤šOCRé”™è¯¯
        3. æ—¶é—´é—´éš”åˆå¹¶ - çŸ­æ—¶é—´å†…çš„ç›¸åŒæ–‡æœ¬ä¼šè¢«åˆå¹¶
        4. æ ‡ç‚¹ç¬¦å·å¤„ç† - å¿½ç•¥æ ‡ç‚¹ç¬¦å·å·®å¼‚

        Args:
            ocr_results: OCRè¯†åˆ«ç»“æœå­—å…¸
            similarity_threshold: æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰ï¼Œé»˜è®¤0.75
            max_gap_seconds: æœ€å¤§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’

        Returns:
            åˆå¹¶åçš„å­—å¹•æ®µåˆ—è¡¨
        """
        if not ocr_results:
            return []

        print(f"æ­£åœ¨åˆå¹¶å­—å¹•æ®µï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}, æœ€å¤§é—´éš”: {max_gap_seconds}ç§’ï¼‰...")

        # æŒ‰å¸§ç´¢å¼•æ’åº
        sorted_results = sorted(ocr_results.items(), key=lambda x: x[1]['frame_index'])

        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºåˆå§‹æ®µè½ï¼ˆåŒ…å«ç©ºæ–‡æœ¬æ®µï¼‰
        initial_segments = []
        for frame_path, value in sorted_results:
            text = value['text'].strip()
            frame_idx = value['frame_index']

            initial_segments.append({
                'frame_index': frame_idx,
                'text': text,
                'is_empty': not text
            })

        # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½åˆå¹¶ç®—æ³•
        segments = []
        current_segment = None
        text_variants = []

        for i, seg in enumerate(initial_segments):
            text = seg['text']
            frame_idx = seg['frame_index']
            is_empty = seg['is_empty']

            # å¦‚æœæ˜¯ç©ºæ–‡æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ï¼ˆåŸºäºæ—¶é—´é—´éš”ï¼‰
            if is_empty:
                if current_segment is not None:
                    # è®¡ç®—æ—¶é—´é—´éš”
                    time_gap = (frame_idx - current_segment['end_frame']) / self.extract_fps

                    # å¦‚æœæ—¶é—´é—´éš”å¾ˆå°ï¼Œè·³è¿‡è¿™ä¸ªç©ºæ–‡æœ¬
                    if time_gap <= max_gap_seconds:
                        # æŸ¥çœ‹ä¸‹ä¸€ä¸ªéç©ºæ–‡æœ¬æ˜¯å¦ä¸å½“å‰æ®µç›¸ä¼¼
                        next_text = self._find_next_non_empty_text(initial_segments, i)
                        if next_text and current_segment:
                            current_text = self._normalize_text(current_segment['text'])
                            next_normalized = self._normalize_text(next_text)
                            similarity = text_similarity(current_text, next_normalized)

                            if similarity >= similarity_threshold:
                                # è·³è¿‡è¿™ä¸ªç©ºæ–‡æœ¬ï¼Œç»§ç»­å½“å‰æ®µ
                                continue

                    # å¦åˆ™ç»“æŸå½“å‰æ®µ
                    self._finalize_current_segment(current_segment, text_variants, segments)
                    current_segment = None
                    text_variants = []
                continue

            # å¤„ç†éç©ºæ–‡æœ¬
            normalized_text = self._normalize_text(text)

            if current_segment is None:
                # å¼€å§‹æ–°æ®µ
                current_segment = {
                    'start_frame': frame_idx,
                    'end_frame': frame_idx,
                    'text': text
                }
                text_variants = [text]
            else:
                # è®¡ç®—ä¸å½“å‰æ®µçš„ç›¸ä¼¼åº¦
                current_normalized = self._normalize_text(current_segment['text'])
                similarity = text_similarity(normalized_text, current_normalized)

                # è®¡ç®—æ—¶é—´é—´éš”
                time_gap = (frame_idx - current_segment['end_frame']) / self.extract_fps

                # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                should_merge = (similarity >= similarity_threshold) and (time_gap <= max_gap_seconds)

                if should_merge:
                    # å»¶é•¿å½“å‰æ®µ
                    current_segment['end_frame'] = frame_idx
                    text_variants.append(text)
                else:
                    # ç»“æŸå½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
                    self._finalize_current_segment(current_segment, text_variants, segments)

                    current_segment = {
                        'start_frame': frame_idx,
                        'end_frame': frame_idx,
                        'text': text
                    }
                    text_variants = [text]

        # ä¿å­˜æœ€åä¸€æ®µ
        if current_segment:
            self._finalize_current_segment(current_segment, text_variants, segments)

        print(f"åˆå¹¶åå¾—åˆ° {len(segments)} ä¸ªå­—å¹•æ®µ")
        return segments

    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬ï¼Œç¹ä½“è½¬ç®€ä½“ï¼Œå»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ"""
        import re

        # 1. ç¹ä½“è½¬ç®€ä½“
        if self.cc and text:
            text = self.cc.convert(text)

        # 2. å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        normalized = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
        return normalized.lower()

    def _convert_to_simplified(self, text: str) -> str:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡"""
        if self.cc and text:
            return self.cc.convert(text)
        return text

    def _is_likely_subtitle_by_geometry(self, box_coords: list, text: str, img_height: int, debug_print: bool = False) -> bool:
        """
        åŸºäºå‡ ä½•ç‰¹å¾åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯å­—å¹•

        Args:
            box_coords: æ–‡æœ¬è¾¹ç•Œæ¡†åæ ‡ [x1, y1, x2, y2]
            text: æ–‡æœ¬å†…å®¹
            img_height: å›¾ç‰‡é«˜åº¦ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹é«˜åº¦ï¼‰
            debug_print: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯

        Returns:
            bool: Trueè¡¨ç¤ºå¯èƒ½æ˜¯å­—å¹•ï¼ŒFalseè¡¨ç¤ºå¯èƒ½æ˜¯å™ªå£°
        """
        if len(box_coords) < 4:
            return True  # å¦‚æœåæ ‡ä¸å®Œæ•´ï¼Œé»˜è®¤é€šè¿‡

        x1, y1, x2, y2 = box_coords[:4]
        width = x2 - x1
        height = y2 - y1
        char_count = len(text)

        if width <= 0 or height <= 0 or char_count == 0:
            return True  # é¿å…é™¤é›¶é”™è¯¯ï¼Œé»˜è®¤é€šè¿‡

        # è®¡ç®—å‡ ä½•ç‰¹å¾
        aspect_ratio = width / height
        avg_char_width = width / char_count
        relative_height = height / img_height  # ä½¿ç”¨å®é™…å›¾ç‰‡é«˜åº¦è®¡ç®—ç›¸å¯¹é«˜åº¦

        # åŸºäºåˆ†æç»“æœçš„è¿‡æ»¤è§„åˆ™
        is_wide_text = aspect_ratio > 1.6  # å®½é«˜æ¯”å¤§äº1.6ï¼ˆå­—å¹•é€šå¸¸æ›´å®½æ‰ï¼‰
        is_reasonable_height = relative_height < 0.4  # ç›¸å¯¹é«˜åº¦å°äº40%
        is_reasonable_char_width = avg_char_width < 100  # å¹³å‡å­—ç¬¦å®½åº¦å°äº100px

        # ç‰¹æ®Šè§„åˆ™ï¼šè¿‡æ»¤æ˜æ˜¾çš„è½¦ç‰Œæ¨¡å¼
        is_license_plate = self._is_license_plate_pattern(text)

        # å•å­—ç¬¦ç‰¹æ®Šå¤„ç†ï¼šå¯¹äºå•ä¸ªæ±‰å­—ï¼Œæ”¾å®½å®½é«˜æ¯”è¦æ±‚
        import re
        is_single_char = char_count == 1
        is_chinese_char = bool(re.match(r'^[\u4e00-\u9fff]$', text))

        # ç»¼åˆåˆ¤æ–­
        if is_single_char and is_chinese_char:
            # å•ä¸ªæ±‰å­—ï¼šåªæ£€æŸ¥é«˜åº¦å’Œæ¨¡å¼ï¼Œä¸æ£€æŸ¥å®½é«˜æ¯”
            passes_geometry = is_reasonable_height and is_reasonable_char_width
        else:
            # å¤šå­—ç¬¦ï¼šæ£€æŸ¥æ‰€æœ‰å‡ ä½•ç‰¹å¾
            passes_geometry = is_wide_text and is_reasonable_height and is_reasonable_char_width

        passes_pattern = not is_license_plate

        result = passes_geometry and passes_pattern

        if debug_print:
            print(f"      å‡ ä½•åˆ†æ: å®½é«˜æ¯”={aspect_ratio:.2f}, ç›¸å¯¹é«˜åº¦={relative_height:.3f}, å­—ç¬¦å®½åº¦={avg_char_width:.1f}")
            print(f"      è§„åˆ™æ£€æŸ¥: å®½æ‰={is_wide_text}, é«˜åº¦åˆç†={is_reasonable_height}, å­—ç¬¦åˆç†={is_reasonable_char_width}")
            print(f"      æ¨¡å¼æ£€æŸ¥: éè½¦ç‰Œ={not is_license_plate}")
            print(f"      æœ€ç»ˆç»“æœ: {'é€šè¿‡' if result else 'è¿‡æ»¤'}")

        return result

    def _is_license_plate_pattern(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè½¦ç‰Œå·ç æ¨¡å¼"""
        import re
        # ä¸­å›½è½¦ç‰Œæ ¼å¼ï¼šåœ°åŒºç +å­—æ¯+æ•°å­—
        license_pattern = r'^[äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†æµ·][A-Z]Â·?\d+$'
        return bool(re.match(license_pattern, text))


    def _ocr_image(self, img, debug_print: bool = False) -> Dict:
        """
        æ ¸å¿ƒçš„å•å¼ å›¾ç‰‡OCRè¯†åˆ«é€»è¾‘

        Args:
            img: OpenCVå›¾ç‰‡å¯¹è±¡ (numpy.ndarray)
            debug_print: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯

        Returns:
            Dict: åŒ…å«è¯†åˆ«ç»“æœçš„å­—å…¸
            {
                'texts': [{'text': str, 'simplified_text': str, 'score': float, 'box': list}],
                'combined_text': str,
                'raw_result': OCRResult
            }
        """
        # è·å–å›¾ç‰‡é«˜åº¦ç”¨äºå‡ ä½•ç‰¹å¾è®¡ç®—
        img_height = img.shape[0] if img is not None and hasattr(img, 'shape') else 480

        # è¿›è¡ŒOCRè¯†åˆ«
        if debug_print:
            print("æ­£åœ¨è¿›è¡ŒOCRè¯†åˆ«...")

        try:
            ocr_result = self.ocr.predict(img, use_textline_orientation=True)

            # è§£æOCRç»“æœ
            result_data = {
                'texts': [],
                'combined_text': "",
                'raw_result': ocr_result
            }

            if ocr_result and len(ocr_result) > 0:
                if debug_print:
                    print(f"OCRè¯†åˆ«å®Œæˆï¼Œæ‰¾åˆ° {len(ocr_result)} ä¸ªæ–‡æœ¬åŒºåŸŸ")

                for i, item in enumerate(ocr_result):
                    # å°è¯•ä¸åŒçš„æ–¹å¼è·å–æ•°æ®
                    rec_texts = []
                    rec_scores = []
                    boxes = []

                    # æ–¹æ³•1: å­—å…¸æ–¹å¼
                    if hasattr(item, 'get'):
                        rec_texts = item.get('rec_texts', [])
                        rec_scores = item.get('rec_scores', [])

                        # ä¼˜å…ˆä½¿ç”¨ç°æˆçš„ rec_boxes
                        rec_boxes = item.get('rec_boxes')
                        if rec_boxes is not None and hasattr(rec_boxes, 'shape'):
                            boxes = rec_boxes.tolist()
                        else:
                            # å¤‡é€‰æ–¹æ¡ˆï¼šä» dt_polys è®¡ç®—è¾¹ç•Œæ¡†
                            dt_polys = item.get('dt_polys', [])
                            if dt_polys:
                                for poly in dt_polys:
                                    x_coords = [p[0] for p in poly]
                                    y_coords = [p[1] for p in poly]
                                    xmin = int(min(x_coords))
                                    xmax = int(max(x_coords))
                                    ymin = int(min(y_coords))
                                    ymax = int(max(y_coords))
                                    boxes.append([xmin, ymin, xmax, ymax])
                            else:
                                # æœ€åå°è¯•è·å–boxeså­—æ®µ
                                boxes = item.get('boxes', [])

                    # æ–¹æ³•2: å±æ€§æ–¹å¼
                    if not rec_texts:
                        rec_texts = getattr(item, 'rec_texts', [])
                        rec_scores = getattr(item, 'rec_scores', [])
                        dt_polys = getattr(item, 'dt_polys', [])

                        # å¦‚æœæœ‰æ£€æµ‹æ¡†ï¼Œè½¬æ¢ä¸ºè¾¹ç•Œæ¡†æ ¼å¼
                        if dt_polys:
                            for poly in dt_polys:
                                x_coords = [p[0] for p in poly]
                                y_coords = [p[1] for p in poly]
                                xmin = int(min(x_coords))
                                xmax = int(max(x_coords))
                                ymin = int(min(y_coords))
                                ymax = int(max(y_coords))
                                boxes.append([xmin, ymin, xmax, ymax])
                        else:
                            boxes = getattr(item, 'boxes', [])

                    # æ–¹æ³•3: å°è¯•å…¶ä»–å¯èƒ½çš„å±æ€§å
                    if not rec_texts:
                        # å°è¯•ç›´æ¥è®¿é—®æ–‡æœ¬å†…å®¹
                        if hasattr(item, 'text'):
                            rec_texts = [item.text] if item.text else []
                            rec_scores = [getattr(item, 'score', 1.0)] if item.text else []
                        elif hasattr(item, 'texts'):
                            rec_texts = item.texts
                            rec_scores = getattr(item, 'scores', [1.0] * len(rec_texts))

                    # å¦‚æœboxesä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤çš„boxes
                    if rec_texts and (not boxes or len(boxes) != len(rec_texts)):
                        boxes = [[0, 0, 100, 30] for _ in rec_texts]

                    if rec_texts and rec_scores:
                        for j, (text, score, box) in enumerate(zip(rec_texts, rec_scores, boxes)):
                            if debug_print:
                                print(f"  æ£€æµ‹åˆ°æ–‡æœ¬ {j+1}: \"{text}\" (ç½®ä¿¡åº¦: {score:.3f})")

                            if score > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                                # å‡ ä½•ç‰¹å¾è¿‡æ»¤
                                box_coords = box if isinstance(box, list) else box.tolist() if hasattr(box, 'tolist') else [0, 0, 100, 30]
                                if self._is_likely_subtitle_by_geometry(box_coords, text, img_height, debug_print):
                                    # è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡
                                    simplified_text = self._convert_to_simplified(text)

                                    text_info = {
                                        'text': text,
                                        'simplified_text': simplified_text,
                                        'score': float(score),
                                        'box': box_coords
                                    }
                                    result_data['texts'].append(text_info)

                                    if debug_print:
                                        print(f"    âœ“ é‡‡ç”¨: \"{simplified_text}\" (ç½®ä¿¡åº¦: {score:.3f})")
                                elif debug_print:
                                    print(f"    âŒ å‡ ä½•è¿‡æ»¤: \"{text}\" (ç½®ä¿¡åº¦: {score:.3f})")
                            else:
                                if debug_print:
                                    print(f"    âœ— è·³è¿‡: ç½®ä¿¡åº¦è¿‡ä½ ({score:.3f} < 0.5)")

                # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
                if result_data['texts']:
                    # å…ˆåˆå¹¶æ–‡æœ¬ï¼Œç„¶åå»é™¤æ‰€æœ‰ç©ºç™½ç¬¦
                    combined_text = ''.join([item['simplified_text'] for item in result_data['texts']])
                    # å»é™¤æ‰€æœ‰ç©ºç™½ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
                    combined_text = ''.join(combined_text.split())
                    result_data['combined_text'] = combined_text

                    if debug_print:
                        print(f"\nåˆå¹¶æ–‡æœ¬: \"{combined_text}\"")
                else:
                    if debug_print:
                        print("\næœªè¯†åˆ«åˆ°ä»»ä½•æ–‡æœ¬")
            else:
                if debug_print:
                    print("OCRè¯†åˆ«å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬")

            return result_data

        except Exception as e:
            if debug_print:
                print(f"OCRè¯†åˆ«å¤±è´¥: {e}")
            raise

    def _find_next_non_empty_text(self, segments: List[Dict], start_index: int) -> str:
        """æŸ¥æ‰¾ä¸‹ä¸€ä¸ªéç©ºæ–‡æœ¬"""
        for i in range(start_index + 1, len(segments)):
            if not segments[i]['is_empty']:
                return segments[i]['text']
        return ""

    def _is_likely_same_text(self, text1: str, text2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯åŒä¸€å¥è¯ï¼ˆè€ƒè™‘OCRå¸¸è§é”™è¯¯ï¼‰"""
        if not text1 or not text2:
            return False

        # é•¿åº¦å·®å¼‚å¤ªå¤§ï¼Œä¸å¤ªå¯èƒ½æ˜¯åŒä¸€å¥è¯
        if abs(len(text1) - len(text2)) > max(len(text1), len(text2)) * 0.3:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å…±åŒå­—ç¬¦
        common_chars = set(text1) & set(text2)
        min_len = min(len(text1), len(text2))

        return len(common_chars) >= min_len * 0.6


    def _finalize_current_segment(self, current_segment: Dict, text_variants: List[str], segments: List[Dict]):
        """å®Œæˆå½“å‰æ®µè½å¹¶æ·»åŠ åˆ°ç»“æœä¸­"""
        if not current_segment:
            return

        # é€‰æ‹©æœ€ä½³æ–‡æœ¬ï¼ˆå‡ºç°æ¬¡æ•°æœ€å¤šï¼Œæˆ–æœ€é•¿çš„ï¼‰
        if text_variants:
            # ä¼˜å…ˆé€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„
            text_counts = {}
            for text in text_variants:
                normalized = self._normalize_text(text)
                if normalized not in text_counts:
                    text_counts[normalized] = []
                text_counts[normalized].append(text)

            # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç»„ï¼Œç„¶åé€‰æ‹©è¯¥ç»„ä¸­æœ€é•¿çš„æ–‡æœ¬
            best_group = max(text_counts.values(), key=len)
            final_text = max(best_group, key=len)
        else:
            final_text = current_segment['text']

        # è®¡ç®—æ—¶é—´æˆ³
        start_time = current_segment['start_frame'] / self.extract_fps + self.start_time
        end_time = (current_segment['end_frame'] + 1) / self.extract_fps + self.start_time

        segments.append({
            'text': final_text,
            'start_time': start_time,
            'end_time': end_time
        })

    def generate_raw_segments(self, ocr_results: Dict[str, Dict]) -> List[Dict]:
        """
        ç”Ÿæˆæœªåˆå¹¶çš„åŸå§‹å­—å¹•æ®µï¼ˆæ¯å¸§ä¸€ä¸ªæ®µè½ï¼‰
        ç”¨äºè°ƒè¯•å’Œå¯¹æ¯”

        Args:
            ocr_results: OCRè¯†åˆ«ç»“æœå­—å…¸

        Returns:
            åŸå§‹å­—å¹•æ®µåˆ—è¡¨
        """
        if not ocr_results:
            return []

        segments = []
        sorted_results = sorted(ocr_results.items(), key=lambda x: x[1]['frame_index'])

        for frame_path, value in sorted_results:
            text = value['text'].strip()
            frame_idx = value['frame_index']

            if not text:
                continue

            # æ¯å¸§ä½œä¸ºç‹¬ç«‹çš„æ®µè½
            start_time = frame_idx / self.extract_fps + self.start_time
            end_time = (frame_idx + 1) / self.extract_fps + self.start_time

            segments.append({
                'text': text,
                'start_time': start_time,
                'end_time': end_time
            })

        return segments

    def format_timestamp(self, seconds: float) -> str:
        """
        å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼

        Args:
            seconds: ç§’æ•°

        Returns:
            SRTæ ¼å¼æ—¶é—´å­—ç¬¦ä¸² (HH:MM:SS,mmm)
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)

        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    def generate_srt(self, segments: List[Dict], output_path: str):
        """
        ç”ŸæˆSRTå­—å¹•æ–‡ä»¶

        Args:
            segments: å­—å¹•æ®µåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"æ­£åœ¨ç”ŸæˆSRTæ–‡ä»¶: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            for idx, segment in enumerate(segments, 1):
                start_time = self.format_timestamp(segment['start_time'])
                end_time = self.format_timestamp(segment['end_time'])
                text = segment['text']

                f.write(f"{idx}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{text}\n")
                f.write("\n")

        print(f"SRTæ–‡ä»¶å·²ä¿å­˜")

    def process_video(self, video_path: str, output_srt_path: str = None, debug_raw: bool = False):
        """
        å¤„ç†è§†é¢‘å¹¶ç”ŸæˆSRTå­—å¹•

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_srt_path: è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            debug_raw: æ˜¯å¦è¾“å‡ºæœªåˆå¹¶çš„åŸå§‹OCRç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        video_path = Path(video_path)

        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„å¹¶è½¬æ¢ä¸º Path å¯¹è±¡
        if output_srt_path is None:
            output_srt_path = self.output_dir / f"{video_path.stem}.srt"
        else:
            output_srt_path = Path(output_srt_path)

        try:
            # æ­¥éª¤1: è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.get_video_info(str(video_path))

            # æ­¥éª¤2: æå–è§†é¢‘å¸§
            frame_count = self.extract_frames(str(video_path), video_info['fps'])

            if frame_count == 0:
                raise RuntimeError("æœªèƒ½æå–ä»»ä½•è§†é¢‘å¸§")

            # æ­¥éª¤3: OCRè¯†åˆ«
            ocr_results = self.ocr_frames()

            # æ­¥éª¤4: æ£€æŸ¥å’Œä¼˜åŒ–OCRç»“æœ
            print("æ­£åœ¨æ£€æŸ¥OCRç»“æœ...")
            ocr_results = self.check_ocr_result(ocr_results, video_info)

            # æ­¥éª¤4.5: å¦‚æœéœ€è¦ï¼Œè¾“å‡ºåŸå§‹æœªåˆå¹¶çš„è°ƒè¯•æ–‡ä»¶
            if debug_raw:
                raw_output_path = output_srt_path.parent / f"{output_srt_path.stem}_raw.srt"
                print(f"\nç”ŸæˆåŸå§‹OCRè°ƒè¯•æ–‡ä»¶: {raw_output_path}")
                raw_segments = self.generate_raw_segments(ocr_results)
                self.generate_srt(raw_segments, str(raw_output_path))
                print(f"åŸå§‹æ®µè½æ•°: {len(raw_segments)} ä¸ª")

            # æ­¥éª¤5: åˆå¹¶å­—å¹•æ®µ
            segments = self.merge_subtitle_segments(ocr_results)

            # æ­¥éª¤6: ç”ŸæˆSRTæ–‡ä»¶
            self.generate_srt(segments, str(output_srt_path))

            print(f"\nå¤„ç†å®Œæˆï¼")
            print(f"SRTæ–‡ä»¶: {output_srt_path}")

            # æ¸…ç†ä¸´æ—¶å¸§æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            # shutil.rmtree(self.frames_dir)

        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
            raise

    def ocr_single_image(self, image_path: str, crop_region: bool = False, save_result: bool = False) -> Dict:
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«

        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            crop_region: æ˜¯å¦è£å‰ªåˆ°å­—å¹•åŒºåŸŸ
            save_result: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Returns:
            OCRè¯†åˆ«ç»“æœ
        """
        import cv2
        from pathlib import Path

        image_path = Path(image_path)
        if not image_path.exists():
            raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

        print(f"æ­£åœ¨è¯†åˆ«å›¾ç‰‡: {image_path}")

        # è¯»å–å›¾ç‰‡
        img = cv2.imread(str(image_path))
        if img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶: {image_path}")

        height, width = img.shape[:2]
        print(f"å›¾ç‰‡å°ºå¯¸: {width}x{height}")

        # å¦‚æœéœ€è¦è£å‰ªåˆ°å­—å¹•åŒºåŸŸ
        if crop_region:
            # è®¡ç®—å­—å¹•åŒºåŸŸ
            bottom_y = int(height * (1 - self.subtitle_region_bottom))
            top_y = int(height * (1 - self.subtitle_region_top))
            crop_height = bottom_y - top_y

            print(f"å­—å¹•åŒºåŸŸ: y={top_y} åˆ° y={bottom_y} (é«˜åº¦={crop_height}px, å æ¯”{self.subtitle_region_bottom*100:.1f}%-{self.subtitle_region_top*100:.1f}%)")

            # è£å‰ªå›¾ç‰‡
            cropped_img = img[top_y:bottom_y, 0:width]

            # ä¿å­˜è£å‰ªåçš„å›¾ç‰‡ç”¨äºè°ƒè¯•
            crop_path = image_path.parent / f"{image_path.stem}_cropped{image_path.suffix}"
            cv2.imwrite(str(crop_path), cropped_img)
            print(f"è£å‰ªåçš„å›¾ç‰‡å·²ä¿å­˜åˆ°: {crop_path}")

            # ä½¿ç”¨è£å‰ªåçš„å›¾ç‰‡è¿›è¡ŒOCR
            ocr_img = cropped_img
        else:
            print("ä½¿ç”¨å®Œæ•´å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«")
            ocr_img = img

        # ä½¿ç”¨æŠ½è±¡çš„æ ¸å¿ƒOCRè¯†åˆ«æ–¹æ³•
        try:
            core_result = self._ocr_image(ocr_img, debug_print=True)

            # æ„å»ºå®Œæ•´çš„ç»“æœæ•°æ®
            result_data = {
                'image_path': str(image_path),
                'image_size': f"{width}x{height}",
                'cropped': crop_region,
                'texts': core_result['texts'],
                'combined_text': core_result['combined_text'],
                'raw_result': core_result['raw_result']
            }

            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if save_result:
                result_file = image_path.parent / f"{image_path.stem}_ocr_result.json"

                # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœ
                save_data = {
                    'image_path': result_data['image_path'],
                    'image_size': result_data['image_size'],
                    'cropped': result_data['cropped'],
                    'combined_text': result_data['combined_text'],
                    'texts': result_data['texts'],
                    'text_count': len(result_data['texts'])
                }

                import json
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, ensure_ascii=False, indent=2)

                print(f"è¯†åˆ«ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

                # åŒæ—¶ä¿å­˜ç®€å•çš„æ–‡æœ¬æ–‡ä»¶
                text_file = image_path.parent / f"{image_path.stem}_ocr_result.txt"
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(f"å›¾ç‰‡: {image_path}\n")
                    f.write(f"å°ºå¯¸: {result_data['image_size']}\n")
                    f.write(f"è£å‰ª: {'æ˜¯' if crop_region else 'å¦'}\n")
                    f.write(f"è¯†åˆ«æ–‡æœ¬æ•°: {len(result_data['texts'])}\n\n")
                    f.write(f"åˆå¹¶æ–‡æœ¬: {result_data['combined_text']}\n\n")
                    f.write("è¯¦ç»†ç»“æœ:\n")
                    for i, text_info in enumerate(result_data['texts'], 1):
                        f.write(f"{i}. \"{text_info['simplified_text']}\" (ç½®ä¿¡åº¦: {text_info['score']:.3f})\n")

                print(f"æ–‡æœ¬ç»“æœå·²ä¿å­˜åˆ°: {text_file}")

            return result_data

        except Exception as e:
            print(f"OCRè¯†åˆ«å¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨PaddleOCRä»è§†é¢‘ä¸­æå–å­—å¹•å¹¶ç”ŸæˆSRTæ–‡ä»¶'
    )
    # åˆ›å»ºäº’æ–¥ç»„ï¼šè§†é¢‘å¤„ç† vs å•å›¾OCR
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        'video',
        nargs='?',
        type=str,
        help='è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„'
    )
    group.add_argument(
        '--ocr-image',
        type=str,
        help='å•å¼ å›¾ç‰‡OCRè¯†åˆ«æ¨¡å¼ï¼šæŒ‡å®šå›¾ç‰‡æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: output/<è§†é¢‘å>.srtï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='output',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰'
    )
    parser.add_argument(
        '--fps',
        type=int,
        default=30,
        help='æå–å¸§ç‡ï¼Œæ¯ç§’æå–å¤šå°‘å¸§ï¼ˆé»˜è®¤: 30ï¼‰'
    )
    parser.add_argument(
        '--subtitle-bottom',
        type=float,
        default=0.2,
        help='å­—å¹•åŒºåŸŸåº•éƒ¨ä½ç½®ï¼Œè·ç¦»åº•éƒ¨çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤: 0.1ï¼Œå³10%%ï¼‰'
    )
    parser.add_argument(
        '--subtitle-top',
        type=float,
        default=0.45,
        help='å­—å¹•åŒºåŸŸé¡¶éƒ¨ä½ç½®ï¼Œè·ç¦»åº•éƒ¨çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤: 0.45ï¼Œå³45%%ï¼‰'
    )
    parser.add_argument(
        '--gpu',
        action='store_true',
        default=True,
        help='ä½¿ç”¨GPUåŠ é€Ÿï¼ˆé»˜è®¤å¼€å¯ï¼Œéœ€è¦å®‰è£…paddlepaddle-gpuï¼‰'
    )
    parser.add_argument(
        '--cpu',
        action='store_true',
        help='å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼'
    )
    parser.add_argument(
        '--preview',
        action='store_true',
        help='ä»…é¢„è§ˆå­—å¹•åŒºåŸŸï¼Œä¸è¿›è¡ŒOCRè¯†åˆ«ï¼ˆç”¨äºè°ƒè¯•å­—å¹•ä½ç½®ï¼‰'
    )
    parser.add_argument(
        '--preview-times',
        type=str,
        default=None,
        help='é¢„è§ˆæ¨¡å¼ä¸‹çš„æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚ "10,30,60,90"ã€‚ä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©å¤šä¸ªæ—¶é—´ç‚¹'
    )
    parser.add_argument(
        '--start-time',
        type=float,
        default=0,
        help='å¼€å§‹å¤„ç†çš„æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: 0ï¼ˆä»å¤´å¼€å§‹ï¼‰'
    )
    parser.add_argument(
        '--duration',
        type=float,
        default=None,
        help='å¤„ç†çš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: Noneï¼ˆå¤„ç†åˆ°è§†é¢‘ç»“æŸï¼‰'
    )
    parser.add_argument(
        '--debug-raw',
        action='store_true',
        help='è¾“å‡ºæœªåˆå¹¶çš„åŸå§‹OCRç»“æœåˆ° *_raw.srt æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰'
    )
    parser.add_argument(
        '--crop-region',
        action='store_true',
        help='å•å›¾OCRæ¨¡å¼ï¼šæ˜¯å¦è£å‰ªåˆ°å­—å¹•åŒºåŸŸï¼ˆé»˜è®¤å¤„ç†æ•´å¼ å›¾ç‰‡ï¼‰'
    )
    parser.add_argument(
        '--save-result',
        action='store_true',
        help='å•å›¾OCRæ¨¡å¼ï¼šä¿å­˜è¯†åˆ«ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶'
    )

    args = parser.parse_args()

# åˆ›å»ºæå–å™¨ï¼ˆç”¨äºOCRåŠŸèƒ½ï¼‰
    extractor = VideoSubtitleExtractor(
        output_dir=args.output_dir,
        extract_fps=args.fps,
        subtitle_region_bottom=args.subtitle_bottom,
        subtitle_region_top=args.subtitle_top,
        use_gpu=not args.cpu,
        start_time=0,
        duration=None
    )

    # æ£€æŸ¥æ˜¯å¦æ˜¯å•å›¾OCRæ¨¡å¼
    if args.ocr_image:
        print("=" * 50)
        print("å•å¼ å›¾ç‰‡OCRè¯†åˆ«æ¨¡å¼")
        print("=" * 50)

        try:
            # æ‰§è¡Œå•å›¾OCR
            result = extractor.ocr_single_image(
                image_path=args.ocr_image,
                crop_region=args.crop_region,
                save_result=args.save_result
            )

            print("\n" + "=" * 50)
            print("OCRè¯†åˆ«å®Œæˆ")
            print("=" * 50)

            if result['combined_text']:
                print(f"âœ“ è¯†åˆ«æˆåŠŸ: \"{result['combined_text']}\"")
            else:
                print("âœ— æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡æœ¬")

        except Exception as e:
            print(f"âœ— OCRè¯†åˆ«å¤±è´¥: {e}")
            return 1

        return 0

    # éªŒè¯è§†é¢‘æ¨¡å¼çš„å‚æ•°
    if not args.video:
        parser.error("è§†é¢‘æ¨¡å¼éœ€è¦æä¾›è§†é¢‘æ–‡ä»¶è·¯å¾„")

    # éªŒè¯å­—å¹•åŒºåŸŸå‚æ•°
    if args.subtitle_bottom < 0 or args.subtitle_bottom > 1:
        parser.error("--subtitle-bottom å¿…é¡»åœ¨ 0 å’Œ 1 ä¹‹é—´")
    if args.subtitle_top < 0 or args.subtitle_top > 1:
        parser.error("--subtitle-top å¿…é¡»åœ¨ 0 å’Œ 1 ä¹‹é—´")
    if args.subtitle_bottom >= args.subtitle_top:
        parser.error("--subtitle-bottom å¿…é¡»å°äº --subtitle-top")

    # éªŒè¯æ—¶é—´å‚æ•°
    if args.start_time < 0:
        parser.error("--start-time å¿…é¡» >= 0")
    if args.duration is not None and args.duration <= 0:
        parser.error("--duration å¿…é¡» > 0")

    # å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼
    if args.preview:
        print("=" * 50)
        print("å­—å¹•åŒºåŸŸé¢„è§ˆæ¨¡å¼")
        print("=" * 50)
        extractor.output_dir.mkdir(parents=True, exist_ok=True)
        extractor.frames_dir.mkdir(parents=True, exist_ok=True)

        # è§£æé¢„è§ˆæ—¶é—´ç‚¹
        preview_times = None
        if args.preview_times:
            try:
                preview_times = [float(t.strip()) for t in args.preview_times.split(',')]
                print(f"ä½¿ç”¨æŒ‡å®šçš„æ—¶é—´ç‚¹: {preview_times}")
            except ValueError:
                print(f"è­¦å‘Š: æ— æ³•è§£ææ—¶é—´ç‚¹ '{args.preview_times}'ï¼Œå°†è‡ªåŠ¨é€‰æ‹©æ—¶é—´ç‚¹")

        extractor.preview_subtitle_region(args.video, preview_times)
        print("\næç¤ºï¼šæ£€æŸ¥ç”Ÿæˆçš„å›¾ç‰‡ï¼Œå¦‚æœå­—å¹•ä½ç½®ä¸å¯¹ï¼Œè¯·è°ƒæ•´ --subtitle-bottom å’Œ --subtitle-top å‚æ•°")
        return

    # å¤„ç†è§†é¢‘
    extractor.process_video(args.video, args.output, debug_raw=args.debug_raw)


if __name__ == "__main__":
    main()
